{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 479: Machine Learning (Fall 2018)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat479-fs2018/\n",
    "\n",
    "**Due**: Nov 08, before class (before 8:00 am).\n",
    "\n",
    "**How to submit**\n",
    "\n",
    "As mentioned in the lecture, you need to submit the `.ipynb` file with your answers plus an `.html` file, which will serve as a backup for us in case the `.ipynb` file cannot be opened on my or the TA's computer. In addition, you may also export the notebook as PDF and upload it as well.\n",
    "\n",
    "This time, we will be using the Canvas platform, so you need to submit your homework there. You should be able to resubmit the homework as many times as you like before the due date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are highly encouraged to use Piazza to ask questions and help each other while working on the homework. However, do not share any solutions with other students as this would be a violation of the Academic Integrity guidelines (for more info, see http://pages.stat.wisc.edu/~sraschka/teaching/stat479-fs2018/#other-important-course-information)**\n",
    "\n",
    "\n",
    "For example, a resonable question & answer would be:\n",
    "\n",
    "- Q: When I am asked to implement the code for majority voting, my code produces an array that has the wrong dimensions (I get the following dimensions ...). \n",
    "- A: Hm, I suspect you compute the `argmax` over rows, not columns. Maybe check that you specify the correct dimension for the `axis` parameter in the `argmax` function.\n",
    "\n",
    "Not ok would be:\n",
    "\n",
    "- Q: Here is my code and solution for exercise XXX. Is this correct? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Your Name> \n",
      "last updated: 2018-10-21 \n",
      "\n",
      "CPython 3.6.6\n",
      "IPython 6.5.0\n",
      "\n",
      "numpy 1.15.1\n",
      "scipy 1.1.0\n",
      "matplotlib 2.2.3\n",
      "sklearn 0.20.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a '<Your Name>' -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Implementing an ID3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the homework, you are going to implement the ID3 decision tree algorithm we discussed in class. This decision tree algorithm will support multi-category splits, but just like the original ID3 algorithm, it will only support categorical feature values for simplicity. Here, categorical feature values will be represented by integer numbers. \n",
    "\n",
    "\n",
    "Implementing machine learning algorithms from scratch is a very important skill, and this homework will provide exercises that will help you to develop this skill. Even if you are interested in the more theoretical aspects of machine learning, being comfortable with implementing and trying out algorithms is vital for doing research, since even the more theoretical papers in machine learning are usually accompanied by experiments or simulations to a) verify results and b) to compare algorithms with the state-of-the art.\n",
    "\n",
    "Since many students are not expert Python programmers (yet), I will provide partial solutions to the homework tasks such that you have a framework or guide to implement the solutions. Areas that you need to fill in will be marked with comments (e.g., `# your code`). For these partial solutions, I first implemented the functions myself, and then I deleted parts you need to fill in by these comments. However, note that you can, of course, use more or fewer lines of code than I did. In other words, all that matter is that the function you write can create the same outputs as the ones I provide. How many lines of code you need to implement that function, and how efficient it is, does not matter here. The expected outputs for the respective functions will be provided so that you can double-check your solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Splitting a node (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to implement a function that splits a dataset along a feature axis into sub-datasets. Since we are going to implement a decision tree that only supports categorical features (like ID3) for simplicity, you do not need to account for continuous feature variables. In other words, the splitting function only needs to support integer NumPy arrays.  \n",
    "\n",
    "To provide an intuitive example, suppose you are given the following NumPy array with four feature values, feature values 0-3:\n",
    "\n",
    "    np.array([0, 1, 2, 1, 0, 3, 1, 0, 1, 2])\n",
    "    \n",
    "The function you are going to implement should return a dictionary, where each dictionary key represents a unique value in the array, and the values are the indices in that array that map to the respective feature value. Hence, based on the feature array above, your `split` function should return the following dictionary:\n",
    "\n",
    "    {0: array([0, 4, 7]), \n",
    "     1: array([1, 3, 6, 8]), \n",
    "     2: array([2, 9]), \n",
    "     3: array([5])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: I recommend you to use `np.where` and `np.unique` functions to make the implementation easier. If you do not remember these functions from the \"computational foundations\" lectures, you can either look up those functions in the NumPy documentation online, or you can execute `np.where?` and `np.unique?` in a new code cell to get more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(array):\n",
    "    # your code to generate dictionary\n",
    "    return # return the dictionary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0]), 1: array([1]), 2: array([2])}\n",
      "{0: array([1, 3, 4, 6]), 1: array([0, 2, 5])}\n",
      "{0: array([1, 4]), 1: array([0, 5, 6]), 2: array([3]), 3: array([2])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(split(np.array([0, 1, 2])))\n",
    "print(split(np.array([1, 0, 1, 0, 0, 1, 0])))\n",
    "print(split(np.array([1, 0, 3, 2, 0, 1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2) Implement Entropy (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the splitting function, we are now have to implement a criterion function so that we can compare splits on different features, to decide which feature is the best feature to split for growing the decision tree. As discussed in class, our splitting criterion will be Information Gain. However, before we implement an Information Gain function, we need to implement a function that computes the entropy at each node, which we need to compute Information Gain.\n",
    "\n",
    "For your reference, we defined entropy (i.e., Shannon Entropy) as follows:\n",
    "\n",
    "$$H(p) = \\sum_i p_i \\log_2 (1/p_i) = - \\sum_i p_i \\log_2 (p_i)$$\n",
    "\n",
    "where you can think of $p_i$ as the proportion of examples with class label $i$ at a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(array):\n",
    "    # your code\n",
    "    # your code\n",
    "    return # return a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `entropy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.4395\n",
      "0.0\n",
      "1.6577\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(round(entropy(np.array([0, 1, 0, 1, 1, 0])), 4))\n",
    "print(round(entropy(np.array([1, 2])), 4))\n",
    "print(round(entropy(np.array([1, 1])), 4))\n",
    "print(round(entropy(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([1, 1, 1, 0, 1, 4, 4, 2, 1])), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Implement Information Gain (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working solution for the `entropy` function, the next step is to compute the Information Gain. For your reference, information gain is computed as\n",
    "\n",
    "$$GAIN(\\mathcal{D}, x_j) = H(\\mathcal{D}) - \\sum_{v \\in Values(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(x_array, y_array):\n",
    "    parent_entropy = # your code\n",
    "\n",
    "    split_dict = # your code\n",
    "    \n",
    "    for val in split_dict:\n",
    "        freq = # your code\n",
    "        child_entropy = # your code\n",
    "        parent_entropy -= # your code\n",
    "        \n",
    "    return parent_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `information_gain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591\n",
      "0.2516\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x = np.array([0, 1, 0, 1, 0, 1])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))\n",
    "\n",
    "x = np.array([0, 0, 1, 1, 2, 2])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You may notice that these are actually the feature arrays from the midterm exam, Q 14.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Decision Tree Splitting (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have all the main components that we need for implementing the ID3 decision tree algorithm: a `split` function, an `entropy` function, and an `information_gain` function based on the `entropy` function. \n",
    "\n",
    "The next task is combine these functions to recursively split a dataset on its different features to construct a decision tree that separate the examples from different classes well. We will call this function `make_tree`. \n",
    "\n",
    "For simplicity, the decision tree returned by the `make_tree` function will be represented by a Python dictionary. To illustrate this, consider the following dataset:\n",
    "\n",
    "```\n",
    "Inputs:\n",
    " [[0 0]\n",
    " [0 1]\n",
    " [1 0]\n",
    " [1 1]\n",
    " [2 0]\n",
    " [2 1]]\n",
    "\n",
    "Labels:\n",
    " [0 1 0 1 1 1]\n",
    "```\n",
    " \n",
    "This is a dataset with 6 training examples and two features. (Again, this is an example from the midterm exam.) The decision tree in form of the Python dictionary should look like as follows:\n",
    "\n",
    "\n",
    "\n",
    "You should return a dictionary with the following form:\n",
    "\n",
    "```\n",
    "{'X_1 = 0': {'X_0 = 0': array([0]),\n",
    "             'X_0 = 1': array([0]),\n",
    "             'X_0 = 2': array([1])},\n",
    " 'X_1 = 1': array([1, 1, 1])}\n",
    " ```\n",
    " \n",
    "Let me further illustrate what the different parts of the dictionary mean. Here, the `'X_1'` in `'X_1 = 0'` refers feature 2 (the first column of the NumPy array; remember that Python starts the index at 0, in contrast to R). \n",
    "\n",
    "- 'X_1 = 0': For training examples stored in this node, the second feature has the value 0\n",
    "- 'X_1 = 1': For training examples stored in this node, the second feature has the value 1\n",
    "\n",
    "The \"array\" is a NumPy array that stores the class labels of the training examples at that node. In the case of `'X_1 = 0'` we actually store actually a sub-dictionary, because this node can be split further. If you have trouble understanding this dictionary representation, the following illustration might help:\n",
    "\n",
    "\n",
    "![](tree-viz-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(X, y):\n",
    "    \n",
    "    # Return array if node is empty or pure (1 example in leaf node)\n",
    "    if y.shape[0] == 1 or y.shape[0] == 0:\n",
    "        return y\n",
    "\n",
    "    # Compute information gain for each feature\n",
    "    gains = # YOUR CODE\n",
    "\n",
    "    # Early stopping if there is no information gain\n",
    "    if (gains <= 1e-05).all():\n",
    "        return # YOUR CODE\n",
    "    \n",
    "    # Else, get best feature\n",
    "    best_feature = np.argmax(gains)\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Use the `split` function to split on the best feature\n",
    "    subset_dict = split(X[:, best_feature])\n",
    "\n",
    "    # Note that each entry in the dictionary returned by \n",
    "    # split is an attribute_value:array_indices pair.\n",
    "    # here, we are going to iterate over these key-value\n",
    "    # pairs and select the respective examples for the\n",
    "    # new child nodes\n",
    "    \n",
    "    for feature_value, train_example_indices in subset_dict.items():\n",
    "        child_y_subset = # YOUR CODE\n",
    "        child_x_subset = # YOUR CODE\n",
    "\n",
    "        # Next, we are using \"recursion,\" that is, calling the same\n",
    "        # tree_split function on the child subset(s)\n",
    "        \n",
    "        results[\"X_%d = %d\" % (best_feature, feature_value)] = \\\n",
    "                make_tree(child_x_subset, child_y_subset)\n",
    "\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 1]]\n",
      "\n",
      "Labels:\n",
      " [0 1 0 1 1 1]\n",
      "\n",
      "Decision tree:\n",
      " {'X_1 = 0': {'X_0 = 0': array([0]), 'X_0 = 1': array([0]), 'X_0 = 2': array([1])}, 'X_1 = 1': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x1 = np.array([0, 0, 1, 1, 2, 2])\n",
    "x2 = np.array([0, 1, 0, 1, 0, 1])\n",
    "X = np.array([x1, x2]).T\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "\n",
    "print('Inputs:\\n', X)\n",
    "print('\\nLabels:\\n', y)\n",
    "\n",
    "print('\\nDecision tree:\\n', make_tree(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Building a Decision Tree API (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of this part of the homework is now to write an API around our decision tree code so that we can use is for making predictions. Here, we will use the common convention, established by scikit-learn, to implement the decision tree as a Python class with \n",
    "\n",
    "- a `fit` method that learns the decision tree model from a training set via the `make_tree` function we already implemented;\n",
    "- a `predict` method to predict the class labels of training examples or any unseen data points.\n",
    "\n",
    "For making predictions, since not all leaf nodes are guaranteed to be single training examples, we will use a majority voting function to predict the class label as discussed in class. I already implemented a `_traverse` method, which will recursively traverse a decision tree dictionary that is produced by the `make_tree` function.\n",
    "\n",
    "Note that for simplicity, the `predict` method will only be able to accept one data point at a time (instead of a collection of data points). Hence `x` is a vector of size $\\mathbb{R}^m$, where $m$ is the number of features. I use capital letters `X` to denote a matrix of size $\\mathbb{R}^{n\\times m}$, where $n$ is the number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3DecisionTreeClassifer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.splits_ = # YOUR CODE to generate the decision tree dictionary\n",
    "        \n",
    "    def _majority_vote(self, label_array):\n",
    "        return # YOUR CODE\n",
    "        \n",
    "    def _traverse(self, x, d):\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return d\n",
    "        for key in d:\n",
    "            name, value = key.split(' = ')\n",
    "            feature_idx = int(name.split('_')[-1])\n",
    "            value = int(value)\n",
    "            if x[feature_idx] == value:\n",
    "                return self._traverse(x, d[key])\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        label_array = # YOUR CODE to get class labels from the target node\n",
    "        return #YOUR CODE to predict the class label via majority voting from label_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "tree = ID3DecisionTreeClassifer()\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict(np.array([0, 0])))\n",
    "print(tree.predict(np.array([0, 1])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 1])))\n",
    "print(tree.predict(np.array([2, 0])))\n",
    "print(tree.predict(np.array([2, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part of this homework, you will be combining multiple decision trees to a bagging classifier. This time, we will be using the decision tree algorithm implemented in scikit-learn (which is some variant of the CART algorithm for binary splits, as discussed in class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bootrapping (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember, bagging relies on bootstrap sampling. So, as a first step, your task is to implement a function for generating bootstrap samples. In this exercise, for simplicity, we will perform the computations based on the Iris dataset.\n",
    "\n",
    "On an interesting side note, scikit-learn recently updated their version of the Iris dataset since it was discovered that the Iris version hosted on the UCI machine learning repository (https://archive.ics.uci.edu/ml/datasets/Iris/) has two data points that are different from R. Fisher's original paper (Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).) and changed it in their most recent version. Since most students may not have the latest scikit-learn version installed, we will be working with the Iris dataset that is deposited on UCI, which has become quite the standard in the Python machine learning community for benchmarking algorithms. Instead of manually downloading it, we will be fetching it through the `mlxtend` (http://rasbt.github.io/mlxtend/) library that you installed in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 150\n",
      "Number of features: 4\n",
      "Unique class labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "from mlxtend.data import iris_data\n",
    "X, y = iris_data()\n",
    "\n",
    "print('Number of examples:', X.shape[0])\n",
    "print('Number of features:', X.shape[1])\n",
    "print('Unique class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's `train_test_split` function to divide the dataset into a training and a test set.\n",
    "\n",
    "- The test set should contain 45 examples, and the training set should contain 105 examples.\n",
    "- To ensure reproducible results, use `123` as a random seed.\n",
    "- Perform a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import # YOUR CODE\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = # YOUR CODE\n",
    "\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are implementing a function to generate bootstrap samples of the training set. In particular, we will perform the bootstrapping as follows:\n",
    "\n",
    "- Create an index array with values 0, ..., 104.\n",
    "- Draw a random sample (with replacement) from this index array using the `choice` method of a NumPy `RandomState` object that is passed to the function as `rng`. \n",
    "- Select training examples from the X array and labels from the y array using the new sample of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bootstrap_sample(rng, X, y):\n",
    "    sample_indices = # YOUR CODE\n",
    "    bootstrap_indices = rng.choice( # YOUR CODE )\n",
    "    return X[# YOUR CODE], y[# YOUR CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `draw_bootstrap_sample` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs from bootstrap round: 105\n",
      "Number of training labels from bootstrap round: 105\n",
      "Labels:\n",
      " [0 0 1 0 0 1 2 0 2 1 0 0 2 1 1 1 1 2 1 1 2 0 2 1 2 1 1 1 0 1 0 0 1 2 0 0 0\n",
      " 0 2 1 1 2 1 2 1 1 2 1 2 0 1 1 2 2 1 0 1 0 2 2 0 1 0 2 0 0 0 0 1 2 0 0 1 0\n",
      " 1 1 0 1 1 2 2 0 2 0 2 0 1 1 2 2 0 2 2 2 0 1 0 1 2 2 2 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "\n",
    "print('Number of training inputs from bootstrap round:', X_boot.shape[0])\n",
    "print('Number of training labels from bootstrap round:', y_boot.shape[0])\n",
    "print('Labels:\\n', y_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Baggging classifier from decision trees (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement a Bagging algorithm based on the `DecisionTreeClassifier`. I provided a partial solution for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class BaggingClassifier(object):\n",
    "    \n",
    "    def __init__(self, num_trees=10, random_state=123):\n",
    "        self.num_trees = num_trees\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [DecisionTreeClassifier(random_state=self.rng) for i in range(self.num_trees)]\n",
    "        for i in range(self.num_trees):\n",
    "            X_boot, y_boot = # YOUR CODE to draw a bootstrap sample\n",
    "            # YOUR CODE to\n",
    "            # fit the trees in self.trees_ on the bootstrap samples\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ary = np.zeros((X.shape[0], len(self.trees_)), dtype=np.int)\n",
    "        for i in range(len(self.trees_)):\n",
    "            ary[:, i] = self.trees_[i].predict(X)\n",
    "\n",
    "        maj = np.apply_along_axis(lambda x:\n",
    "                                  np.argmax(np.bincount(x)),\n",
    "                                            axis=1,\n",
    "                                            arr=ary)\n",
    "        return maj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `BaggingClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Tree Accuracies:\n",
      "88.9%\n",
      "93.3%\n",
      "97.8%\n",
      "93.3%\n",
      "93.3%\n",
      "93.3%\n",
      "91.1%\n",
      "97.8%\n",
      "97.8%\n",
      "97.8%\n",
      "\n",
      "Bagging Test Accuracy: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "model = BaggingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('Individual Tree Accuracies:')\n",
    "for tree in model.trees_:\n",
    "    predictions = tree.predict(X_test) \n",
    "    print('%.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))\n",
    "\n",
    "print('\\nBagging Test Accuracy: %.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Bias-Variance Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will be asked to compute the variance and bias components of the 0-1 loss that we discussed in class. \n",
    "\n",
    "- In particular, you will compute the average bias and the average variance over all test examples (instead of a single test example. \n",
    "\n",
    "- The dataset you will be using as training set(s) and test set is the Iris dataset that you already divided into `X_train` / `y_train` and `X_test` / `y_test` earlier.\n",
    "\n",
    "- Since we do not have unlimited training datasets to estimate the parameters (think back of the estimation over the training sets), we will use bootstrapping to simulate \"new\" training sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bias-Variance decomposition of the 0-1 Loss for Decision Trees (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, you will be computing the averaged bias and variance components over the test set examples for the decision tree algorithm implemented in scikit-learn on the Iris data. \n",
    "\n",
    "I already implemented the code for computing the \"main prediction\" for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "num_bootstrap = 200\n",
    "\n",
    "all_pred = np.zeros((num_bootstrap, y_test.shape[0]), dtype=np.int)\n",
    "\n",
    "for i in range(num_bootstrap):\n",
    "    X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "    pred = DecisionTreeClassifier(random_state=66).fit(X_boot, y_boot).predict(X_test)\n",
    "    all_pred[i] = pred\n",
    "    \n",
    "main_predictions = np.apply_along_axis(lambda x:\n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=all_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `all_pred` is a 2D array of dimension $\\mathbb{R}^{b \\times n_{test}}$, where $m$ is the number of bootstrap rounds and $n_{test}$ is the number of test examples in the test set. In other words, each of the 200 rows in this array stores the predictions of one particular decision tree hypothesis for all 45 test data points.\n",
    "\n",
    "Your first task is to compute the average bias over all test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "\n",
    "print('Average bias:', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second task is to compute the average variance over all test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# you probably need multiple\n",
    "# lines of code and a for-loop\n",
    "\n",
    "print('Average variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The average bias and variance values are both scalars, not vectors or matrices. In other words, for each of the code cells above, you should return a real number (float)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bias-Variance decomposition of the 0-1 Loss for Bagging (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code from the previous section, 3.1, to compare the decision tree algorithm with a BaggingClassifier from scikit-learn.\n",
    "\n",
    "- Report both the average bias and average variance just like before, but use the `BaggingClassifier` in scikit-learn instead of the `DecisionTreeClassifier`. You can use the default values of `BaggingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION\n",
    "# Many lines of code (which you may copy and modify from 3.1)\n",
    "\n",
    "\n",
    "print('Average bias:', bias)\n",
    "print('Average variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the average variance higher or lower than the avergage of the decision tree in 3.1? And what about the average bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! TYPE YOUR ANSWER HERE !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Bias-Variance decomposition of the 0-1 Loss for AdaBoost (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code from the previous section, 3.1, to compare the decision tree algorithm with a AdaBoostClassifier from scikit-learn.\n",
    "\n",
    "- Report both the average bias and average variance just like before, but use the `AdaboostClassifier` in scikit-learn instead of the `DecisionTreeClassifier`. You can use the default values of `AdaboostClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION\n",
    "# Many lines of code (which you may copy and modify from 3.1)\n",
    "\n",
    "\n",
    "\n",
    "print('Average bias:', bias)\n",
    "print('Average variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the average variance higher or lower than the avergage of the decision tree in 3.1? And what about the average bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! TYPE YOUR ANSWER HERE !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this bonus exercise, you will be asked to fit a `RandomForestClassifier` on a small subset (10%) of the MNIST handwritten digits dataset (http://yann.lecun.com/exdb/mnist/). For convenience, the following code loads this small subset via mlxtend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 5000 x 784\n",
      "1st row [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  51. 159. 253. 159.  50.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  48. 238. 252. 252. 252. 237.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  54.\n",
      " 227. 253. 252. 239. 233. 252.  57.   6.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  10.  60. 224.\n",
      " 252. 253. 252. 202.  84. 252. 253. 122.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 163. 252. 252.\n",
      " 252. 253. 252. 252.  96. 189. 253. 167.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 238. 253. 253.\n",
      " 190. 114. 253. 228.  47.  79. 255. 168.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  48. 238. 252. 252. 179.\n",
      "  12.  75. 121.  21.   0.   0. 253. 243.  50.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  38. 165. 253. 233. 208.  84.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 165.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   7. 178. 252. 240.  71.  19.  28.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  57. 252. 252.  63.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 198. 253. 190.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 255. 253. 196.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  76. 246. 252. 112.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 148.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 230.  25.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   7. 135. 253. 186.  12.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 223.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   7. 131. 252. 225.  71.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 145.   0.   0.   0.   0.   0.\n",
      "   0.   0.  48. 165. 252. 173.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  86. 253. 225.   0.   0.   0.   0.   0.\n",
      "   0. 114. 238. 253. 162.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 249. 146.  48.  29.  85. 178.\n",
      " 225. 253. 223. 167.  56.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 252. 252. 229. 215. 252. 252.\n",
      " 252. 196. 130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  28. 199. 252. 252. 253. 252. 252. 233.\n",
      " 145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  25. 128. 252. 253. 252. 141.  37.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.data import mnist_data\n",
    "X, y = mnist_data()\n",
    "\n",
    "print('Dimensions: %s x %s' % (X.shape[0], X.shape[1]))\n",
    "print('1st row', X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell shuffles the dataset and divides it into 4500 training examples and 500 test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import shuffle_arrays_unison\n",
    "\n",
    "\n",
    "X, y = shuffle_arrays_unison((X, y), random_seed=1)\n",
    "X_train, y_train = X[:4500], y[:4500]\n",
    "X_test, y_test = X[4500:], y[4500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to fit a RandomForest classifier on the training set and evaluate it's predictive accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 93.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "model.fit(#YOUR CODE)\n",
    "\n",
    "acc = # YOUR CODE\n",
    "print('Accuracy %.1f%%' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, your task is to load an image of a digit (some_digit.png) from this directory into a Python array and classify it using the random forest model. The some_digit.png image is displayed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](some_digit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For loading the image, you need to install the Python imaging library PIL. Actually, Pillow, a more up-to-date fork is recommended. Execute one of the following two if you haven't installed Pillow already.\n",
    "    \n",
    "- `conda install Pillow`\n",
    "\n",
    "- `pip install Pillow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I have partially pre-written the code for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_image(file_name):\n",
    "    img = Image.open(file_name)\n",
    "    img.load()\n",
    "    data = np.asarray(img, dtype=np.float)\n",
    "    return data\n",
    "\n",
    "x_image = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 5\n"
     ]
    }
   ],
   "source": [
    "# The data needs to be represented as a vector (1 position for each feature)\n",
    "x_transf = # YOUR CODE\n",
    "\n",
    "# Also, scikit-learn expects 2D arrays, so we need to add a dimension\n",
    "x_transf = # YOUR CODE\n",
    "\n",
    "print('Digit:', model.predict(x_transf)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
